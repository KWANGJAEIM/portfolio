{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install 🤗 Transformers and 🤗 Datasets. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "#! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGGev2zodKt-"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Ht8DxxhCdKt-"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M_jzJo-dKt-"
      },
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "y_PTX9MPdKt_"
      },
      "outputs": [],
      "source": [
        "# !apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RopZhcqdKt_"
      },
      "source": [
        "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_RW6vosfdKuA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.24.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFASsisvIrIb"
      },
      "source": [
        "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/question-answering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a question-answering task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoGqzLHNdKuB"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model to a question answering task, which is the task of extracting the answer to a question from a given context. We will see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it.\n",
        "\n",
        "![Widget inference representing the QA task](https://github.com/huggingface/notebooks/blob/main/examples/images/question_answering.png?raw=1)\n",
        "\n",
        "**Note:** This notebook finetunes models that answer question by taking a substring of a context, not by generating new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "This notebook is built to run on any question answering task with the same format as SQUAD (version 1 or 2), with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
        "# answers are allowed or not).\n",
        "squad_v2 = False\n",
        "model_checkpoint = \"skt/kobert-base-v1\"\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "For our example here, we'll use the [SQUAD dataset](https://rajpurkar.github.io/SQuAD-explorer/). The notebook should work with any question answering dataset provided by the 🤗 Datasets library. If you're using your own dataset defined from a JSON or csv file (see the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) on how to load them), it might need some adjustments in the names of the columns used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270,
          "referenced_widgets": [
            "69caab03d6264fef9fc5649bffff5e20",
            "3f74532faa86412293d90d3952f38c4a",
            "50615aa59c7247c4804ca5cbc7945bd7",
            "fe962391292a413ca55dc932c4279fa7",
            "299f4b4c07654e53a25f8192bd1d7bbd",
            "ad04ed1038154081bbb0c1444784dcc2",
            "7c667ad22b5740d5a6319f1b1e3a8097",
            "46c2b043c0f84806978784a45a4e203b",
            "80e2943be35f46eeb24c8ab13faa6578",
            "de5956b5008d4fdba807bae57509c393",
            "931db1f7a42f4b46b7ff8c2e1262b994",
            "6c1db72efff5476e842c1386fadbbdba",
            "ccd2f37647c547abb4c719b75a26f2de",
            "d30a66df5c0145e79693e09789d96b81",
            "5fa26fc336274073abbd1d550542ee33",
            "2b34de08115d49d285def9269a53f484",
            "d426be871b424affb455aeb7db5e822e",
            "160bf88485f44f5cb6eaeecba5e0901f",
            "745c0d47d672477b9bb0dae77b926364",
            "d22ab78269cd4ccfbcf70c707057c31b",
            "d298eb19eeff453cba51c2804629d3f4",
            "a7204ade36314c86907c562e0a2158b8",
            "e35d42b2d352498ca3fc8530393786b2",
            "75103f83538d44abada79b51a1cec09e",
            "f6253931d90543e9b5fd0bb2d615f73a",
            "051aa783ff9e47e28d1f9584043815f5",
            "0984b2a14115454bbb009df71c1cf36f",
            "8ab9dfce29854049912178941ef1b289",
            "c9de740e007141958545e269372780a4",
            "cbea68b25d6d4ba09b2ce0f27b1726d5",
            "5781fc45cf8d486cb06ed68853b2c644",
            "d2a92143a08a4951b55bab9bc0a6d0d3",
            "a14c3e40e5254d61ba146f6ec88eae25",
            "c4ffe6f624ce4e978a0d9b864544941a",
            "1aca01c1d8c940dfadd3e7144bb35718",
            "9fbbaae50e6743f2aa19342152398186",
            "fea27ca6c9504fc896181bc1ff5730e5",
            "940d00556cb849b3a689d56e274041c2",
            "5cdf9ed939fb42d4bf77301c80b8afca",
            "94b39ccfef0b4b08bf2fb61bb0a657c1",
            "9a55087c85b74ea08b3e952ac1d73cbe",
            "2361ab124daf47cc885ff61f2899b2af",
            "1a65887eb37747ddb75dc4a40f7285f2",
            "3c946e2260704e6c98593136bd32d921",
            "50d325cdb9844f62a9ecc98e768cb5af",
            "aa781f0cfe454e9da5b53b93e9baabd8",
            "6bb68d3887ef43809eb23feb467f9723",
            "7e29a8b952cf4f4ea42833c8bf55342f",
            "dd5997d01d8947e4b1c211433969b89b",
            "2ace4dc78e2f4f1492a181bcd63304e7",
            "bbee008c2791443d8610371d1f16b62b",
            "31b1c8a2e3334b72b45b083688c1a20c",
            "7fb7c36adc624f7dbbcb4a831c1e4f63",
            "0b7c8f1939074794b3d9221244b1344d",
            "a71908883b064e1fbdddb547a8c41743",
            "2f5223f26c8541fc87e91d2205c39995"
          ]
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset custom_squad_v2 (C:/Users/sms20/.cache/huggingface/datasets/custom_squad_v2/custom-squad_v2/0.0.1/1001b7d5ce51a8ee107befac51126bc0d7c9e542dd804201d0efe146a178719f)\n"
          ]
        }
      ],
      "source": [
        "# datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "dataset = load_dataset('custom_squad_v2', split='train')\n",
        "datasets = dataset.train_test_split(0.1)\n",
        "datasets['validation'] = datasets['test']\n",
        "del datasets['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "GWiVUF0jIrIv",
        "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 10833\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 1204\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJKqpNBMdKuE"
      },
      "source": [
        "We can see the training, validation and test sets all have a column for the context, the question and the answers to those questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EtYfeHIrIz"
      },
      "source": [
        "To access an actual element, you need to select a split first, then give an index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "X6HrpprwIrIz",
        "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '23c550aaf6fc4df08e1f050c876dbd46',\n",
              " 'title': '유병언 관계사 70곳 … 41개 금융사서 3700억 부당대출 의혹',\n",
              " 'context': '‘신용협동조합의 사금고화, 불법 외화 유출, 분식회계, 대출 자금 불법 유용, 관계사 간 부당 자금 지원….’ 금융감독원의 ‘중앙수사부로’로 불리는 기획검사국 등이 지난달 말부터 156명의 대규모 검사역을 투입해 유병언 전 세모그룹 회장 일가에 대한 특별검사를 벌인 결과다. ‘금융비리 종합세트’라는 말이 나올 정도다. ○‘신협’ 자금줄 의혹 사실로금감원 특검 결과 유 전 회장과 장남 대균, 차남 혁기씨 등 일가는 기독교복음침례회(구원파) 신도들이 세운 일부 신협을 자금줄로 활용했다. 신협은 계(契)처럼 뜻이 맞는 사람들끼리 돈을 모아 조합을 만든 뒤 이 돈을 굴려 수익을 나눠 갖는 구조다. 따라서 내부통제가 다른 금융회사보다 허술하다. 유 전 회장 일가는 이 점을 노렸다. 유 전 회장 일가 4명은 특별한 이유 없이 2006년부터 2012년까지 구원파 신도들이 세운 신협 한 곳으로부터 66억원을 송금받았다. 권순찬 금감원 기획검사국장은 “정기적 송금은 아니었지만 상당히 긴 기간 동안 여러 차례에 걸쳐 송금받은 것으로 확인됐다”고 설명했다. 10여곳의 신협을 낀 관계사 간 부당 자금 지원도 이뤄졌다. 청해진해운 관계사들은 2007년 10월부터 올해 5월까지 신협 대출을 통해 총 727억원을 마련, 다른 관계사에 514억원을 지원했다. 일부 조합원은 신협에서 300만~500만원을 신용대출받아 건강식품 구매 명목으로 다시 소속 교회 계좌로 입금한 것으로 확인됐다. 일부 신협은 관계사인 하니파워에 연체 중인 은행대출(8억2800만원)을 낮은 금리로 대환해주고 연체이자 3000만원을 감면해주는 등 특혜를 제공한 사실도 드러났다. 심지어 관계사인 금수원의 지시로 매년 구원파 신도의 여름수련회 행사비까지 지원한 것으로 밝혀졌다. 유 전 회장의 사진 4장을 1100만원, 사진캘린더 12개를 240만원에 사들이기도 했다. ○3300만달러 불법 외화 유출 혐의 포착불법으로 외화를 빼돌리고 외국환거래법을 수시로 위반한 사실도 확인됐다. 유 전 회장 일가는 해외 현지 법인의 투자 지분을 제3자에 공짜로 넘기거나 헐값으로 처분해 760만달러가량의 투자 자금 회수가 불투명한 상황이다. 천해지 등 관계사는 유 전 회장의 사진 매입 및 저작권료 명목으로 해외 현지 법인에 2570만달러를 송금한 사실도 적발됐다. 유 전 회장 일가는 해외 현지 법인 자회사 설립 및 부동산 투자 과정에서 16건의 외국환거래법을 위반하기도 했다. 위반 액수는 1000만달러가 넘는다. 특수관계자와의 거래내역을 은폐하거나 자산 가격을 부풀리는 등의 분식회계 혐의도 적발됐다. 유 전 회장에게 급여, 컨설팅 비용, 고문료 등을 과다 지급하거나 재고자산을 과대 평가하는 식이었다. 관계사 종업원을 동원한 자금 조성 혐의도 포착됐다. 은행 등 금융회사들이 유 전 회장 일가 및 관계사 등에 내준 대출이 부실 투성이인 사실도 드러났다. 금융회사들은 자금 용도 심사를 생략하고 대출 자금을 용도와 달리 유용했음에도 이를 알지 못했다. 청해진해운 관계사와 관계인이 금융회사로부터 빌린 돈은 모두 3700억원이 넘는 것으로 나타났다. 관계사 70곳 가운데 여신이 있는 46곳의 빚은 3365억원으로 집계됐다. 이들은 은행(13곳)에서 전체 여신액의 83.9%인 2822억원을 빌렸다. 신협 등 상호금융(10곳), 여신전문회사(8곳), 보험사(3곳) 등 모두 41개의 금융회사에서 돈을 차입했다. 청해진해운의 관계인(186명) 중 여신이 있는 90명이 금융회사에서 빌린 금액은 382억원이었다. 권 국장은 “이번 검사에서 적발된 금융사 및 임직원의 부당 행위에 대해 강력히 제재하고 부당 대출금은 회수 조치할 방침”이라고 말했다.',\n",
              " 'question': '유병언의 금융비리에 가담한 종교단체는?',\n",
              " 'answers': {'text': ['기독교복음침례회', '기독교복음침례회(구원파)', '구원파'],\n",
              "  'answer_start': [233, 233, 242]}}"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QKijD--dKuF"
      },
      "source": [
        "We can see the answers are indicated by their start position in the text (here at character 515) and their full text, which is a substring of the context as we mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f77bb7e395d4441eb4bd0a1b9b9a3c14</td>\n",
              "      <td>지금은 여성시대 국산 내시경 만드는 현주인테크 송경애 사장...수리하다 직접 생산 … “명품 내시경 만들겠다”</td>\n",
              "      <td>송경애 현주인테크 사장은 외환위기 직후인 1998년 창업, 이듬해부터 미국의 내시경 수리업체와 제휴해 국내에서 내시경 수리 사업을 시작했다. 2001년부터는 자체 기술로 내시경을 고쳤고, 2009년 연구소를 세워 내시경 개발에 나섰다. 송 대표는 지난달 박근혜 대통령과 함께 독일을 방문한 71명의 중소기업인 가운데 한 명이기도 하다. 현주인테크는 자체 개발 중인 2차원(2D) 위 내시경 제품을 독일에서 전시했다. 그는 “지난해 약 30억원의 매출을 올렸고 올해는 40억원을 목표로 잡았다”며 “국내 시장을 99% 이상 장악한 외국산 제품에 맞설 수 있는 국산 명품 내시경을 만들겠다”고 말했다. ○수리 사업하다 제조업으로송 사장은 성균관대 한문교육학과를 졸업한 뒤 중·고교에서 한문 교사로 사회에 첫발을 내디뎠다. 뭔가 재미있는 일을 하고 싶어 2년 반 만에 학교를 나왔고 광고회사 3년, 보험회사를 3년 다니다 1998년 친구의 권유로 폐쇄회로TV(CCTV)를 활용해 관제시스템을 구축하는 일을 시작했다. “연구소나 병원 등을 많이 방문하다 보니 내시경이 자주 말썽을 부린다는 사실을 알게 됐습니다. 외국산 내시경 수리 비용을 국내 기업들이 아주 비싸게 받고 있었습니다. 그래서 곧바로 사업 아이템을 내시경 수리로 바꿨죠.” 송 사장은 “한 개를 고쳐달라고 우리한테 맡기면 일일이 들여다보고 2개, 3개를 고쳐줬다”며 “온갖 종류의 내시경을 들여다보니까 자연스럽게 구조를 알게 됐고 ‘직접 만들면 어떨까’라는 생각을 하게 됐다”고 설명했다. ○터키·인도네시아 등에 수출송 사장은 오목한 가슴뼈를 교정할 수 있는 내시경(펙토스코프)을 지난해 개발했다. 폐 심장 등 주요 장기를 다치지 않게 하면서 안정적으로 오목한 가슴뼈를 교정할 수 있는 내시경이다. 서울성모병원 흉부외과 연구팀의 요청으로 개발한 제품이다. 작년에 만든 꼬리뼈 내시경은 허리 신경 질환을 치료할 수 있는 제품이다. 이미 터키 인도네시아에 수출했고 말레이시아 우크라이나 시리아 등에서 구매 요청이 들어오고 있다. 그는 “외국인들을 만나면 ‘수리를 하다가 어떻게 내시경을 만들어 팔 수 있느냐’며 놀란다”고 말했다. ○“싸고 좋은 제품 내놓겠다”송 사장이 처음 병원이나 연구소 등에 내시경을 판매하러 다닐 때 ‘당신이 내시경을 알기는 하느냐’며 박대당한 적이 많았다. 그는 그럴 때마다 “우리 제품이 외국 제품보다 품질은 좋고 가격은 싸다”고 계속 설득했다. 현주인테크 직원은 현재 20여명이다. 송 사장은 “오는 7월에는 신장의 아주 작은 결석까지 안전하게 제거할 수 있는 비뇨기과용 내시경을 해외 제품의 절반 가격에 내놓을 예정”이라며 “장기적으로는 침샘, 눈물샘으로 들어가는 작고 정교한 내시경 등 더 가늘고 정확한 제품을 만들겠다”고 포부를 밝혔다.</td>\n",
              "      <td>송경애 사장이 처음으로 갖게되었던 직업은 무엇인가?</td>\n",
              "      <td>{'text': ['한문 교사', '교사'], 'answer_start': [381, 384]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b5acfa6c3c1b43cfa0a31d7e1bbbf2bf</td>\n",
              "      <td>착한 목자회</td>\n",
              "      <td>1976년부터 1994년까지 성 비오 10세회 프랑스 관구장이었던 폴 올라니 신부는 2002년에 성좌와 성 요한 마리아 비안네 직할 서리구 사제들의 일치를 두둔했다가 2003년에 성 비오 10세회에서 제명되었다. 성 요한 마리아 비안네 교구 서리 사제들은 전통적 시각에서 제2차 바티칸 공의회를 인정한다는 것과 교황 바오로 6세가 공포한 개정된 로마 미사 경본의 유효성을 인정한다는 조건으로 성좌와 로마 전례의 트리엔트 양식을 사용하는 것을 허락받았다.\\n\\n2004년 8월 필리프 라게리 신부는 성 비오 10세회 소속 신학교에서 사제들이 제 목소리를 내지 못하는 심각한 문제점이 있다고 지적했다는 이유로 제명당했다. 징계 조치로 그는 멕시코로 전근 조치되었으나 본인이 거부했다. \\n\\n라게리 신부는 1977년부터 수년 동안 전통 가톨릭 신자들이 인수한 프랑스 파리의 생 니콜라 뒤 샤르돈네 성당의 주임 신부를 맡아 왔다. 1993년 그는 파리에 있는 또 다른 성당인 생 제르맹 룩세루아 성당을 인수하려고 시도했다. 그는 대주교의 승인은 아니지만 시 의회의 승인을 받아 보르도에서 2002년 1월 생엘로이 성당을 인수하는데 성공했다. \\n\\n크리스토프 헤리 신부는 라게리 신부를 옹호했다는 이유로 제명됐으며, 프랑스 파리의 생마셸 협회와 생폴 센터의 설립자인 기욤 드 타누아 신부 역시 마찬가지로 제명되었다. \\n\\n2006년 6월 15일 프랑스 낭테르 지방 법원은 라게리 신부와 헤리 신부를 복직하라는 판결을 내렸다.</td>\n",
              "      <td>1977년부터 생 니콜라 뒤 샤르돈네 성당에서 주임 신부로 활동한 사람은?</td>\n",
              "      <td>{'text': ['라게리 신부', '라게리'], 'answer_start': [381, 381]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9b9e9438a73a45dd866c19032fedfec3</td>\n",
              "      <td>예거르쿨트르 올 대표 라인 3...① 세컨드 타임존 分까지 ② 장인의 섬세함 담고 ③ 기술력의 집합</td>\n",
              "      <td>명품시계 브랜드들이 분주하게 움직이고 있다. 올초 해외에서 공개한 신상품이 한국에 본격 입고되면서 이를 소개하기 위한 행사로 바빠지는 시기다. 1833년 탄생한 예거르쿨트르는 지난달 30일 서울 현대백화점 압구정점에서 올해 신상품을 언론에 공개했다. 예거르쿨트르가 야심작으로 꼽은 하이라이트 워치 세 점을 자세히 들여다봤다.‘듀오미터 유니크 트래블 타임’(①)은 출장·여행이 잦은 사람들을 위해 두 지역의 시간대를 동시에 보여주는 월드타임 워치다. 이 제품의 특징은 세컨드 타임 존의 시간을 분(分) 단위까지 정확하게 보여주는 세계 최초의 시계라는 것이다. 일반적으로 시차는 1시간 단위로 벌어지기 때문에 대부분의 월드타임 워치는 세컨드 타임 존에서 시(時)만 표시한다. 하지만 한국보다 3시간15분 느린 네팔이나 30분 단위로 시차가 벌어지는 베네수엘라 미얀마 등처럼 예외가 적지 않다. 듀오미터 유니크 트래블 타임은 이런 지역의 분 단위까지 정확하게 나타내는 흔치 않은 시계다. 가격은 5700만원대.‘마스터 그랑 트래디션 퀀템 퍼페추얼 8데이 SQ’(②)는 내부 부품이 훤히 들여다보이는 스켈레톤 워치다. 다이얼 테두리를 파란 빛깔로 장식해 멋을 더하는데, 자세히 들여다보면 단순한 파란색이 아니라 기요셰(guilloche·금속판에 일정한 무늬를 새겨 넣는 것) 패턴에 반투명 에나멜로 코팅한 정교함을 볼 수 있다. 이 시계는 앞뿐만 아니라 뒤까지 감상해야 한다. 경도와 위도를 연상시키는 곡선을 새겨넣어 3차원의 지구 형태를 완성한 독특한 디자인이 매력적이다. 제품 곳곳에 핸드 드로잉, 베벨링, 폴리싱, 체이싱, 인그레이빙 등 시계 장인들의 수작업으로 완성된 화려한 제조 기술이 엿보인다. 그래서인지 가격도 예사롭지 않다. 1억4000만원대.‘마스터 그랑 트래디션 투르비용 실린더릭 퀀템 퍼페추얼’(③)은 고급 기능을 한데 탑재한 컴플리케이션 워치다. 이번에 핑크 골드 버전으로 새로 나왔다. 9시 방향에는 날짜, 12시 방향에는 연(年)과 월(月), 3시 방향에는 요일을 표시하는 작은 창을 따로 달았다. 6시 방향의 투르비용(중력으로 인한 시간 오차를 줄이는 장치)은 빙글빙글 돌며 신비로운 모습을 연출한다. 가격은 1억6000만원대.</td>\n",
              "      <td>세계 최초로 세컨드 타임 존의 분을 표현한 시계는?</td>\n",
              "      <td>{'text': ['‘듀오미터 유니크 트래블 타임’', '듀오미터 유니크 트래블 타임'], 'answer_start': [181, 182]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>46ed746399df4d54a9c3d3d38af1c1e6</td>\n",
              "      <td>News + 공무원연금 개혁은 ‘용두사미’ 만들어 놓고 …...대책도 없이 국민연금 더 주겠다는 국회</td>\n",
              "      <td>정치권이 정부안보다 크게 후퇴한 공무원연금 개혁안에 합의하면서 난데없이 국민연금 소득대체율을 끌어올리기로 해 논란이 일고 있다. 기금 고갈 문제가 심각해 공적 연금 개혁이 시급한 마당에 재원 마련 계획도 없이 국민연금을 더 지급하겠다는 ‘대책 없는 약속’만 덜컥 한 것이다. ▶관련기사 A4, 5, 6면김무성 새누리당 대표와 유승민 원내대표, 문재인 새정치민주연합 대표와 우윤근 원내대표는 지난 2일 국회에서 만나 ‘더 내고 덜 받는’ 방향의 공무원연금 개혁안을 담은 공무원연금법 개정안을 오는 6일 국회 본회의에서 처리하기로 합의했다. 또 공적 연금 기능을 강화하기 위해 2028년까지 40%로 낮추기로 돼 있는 국민연금의 소득대체율을 50%로 인상하기로 하고, 공무원연금 개혁을 통해 절감하는 재정의 20%를 국민연금에 투입하기로 했다.여야는 공무원연금 기여율(보험료율)은 현 7%에서 9%로 단계적으로 인상하되 연금지급률은 1.9%에서 1.7%로 낮춰 앞으로 70년간 333조원의 재정을 절감하게 됐다고 발표했다. 그러나 정부와 청와대는 국회 개정안이 공무원연금의 근본적인 구조개혁은 도외시한 채 미세조정에 그쳤다는 점에 강하게 반발하고 있다. 기금 고갈 문제로 하향 조정되고 있는 국민연금 소득대체율을 재원 대책도 없이 끌어올리겠다고 발표하면서 국민연금 개혁마저 퇴보하게 됐다는 게 정부의 시각이다. 문형표 보건복지부 장관은 “소득대체율을 50%로 올리면 2050년까지 664조원, 2083년까지 1669조원이 필요하다”며 “정치권에서 섣불리 합의하고 발표할 사안이 아니다”고 말했다.여야 합의안을 발표한 직후 김 대표는 “공무원연금 개혁안 합의는 19대 국회의 가장 큰 업적”이라고 자화자찬했다. 그러나 공론화 과정을 무시한 데다 국민 부담에 대해선 전혀 언급하지 않은 무책임한 발표라고 전문가와 관계기관은 지적했다. 국민연금공단 고위 관계자는 “소득대체율을 높이려면 공론 과정을 거쳐 보험료율과 지급개시연령을 얼마나 높일지부터 결정해야 한다”고 말했다.</td>\n",
              "      <td>국민연금의 소득대체율을 인상하기로 협의한 날짜는?</td>\n",
              "      <td>{'text': ['2일'], 'answer_start': [221]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>efcd5ece0505409591e8e555391a54a5</td>\n",
              "      <td>한경+</td>\n",
              "      <td>갤럭시S5 언제 발매한다는 건지언제는 “27일 판매한다”고 했다가 “이르면 26일 판매한다”고 했다가 지금은 “판매시기 미정”. 삼성전자 갤럭시S5 판매 개시 시기를 놓고 SK텔레콤과 삼성의 말이 엇갈려 소비자들만 골탕. SK는 영업정지가 시작되는 다음달 5일 전에 서둘러 판매하고 싶어하는데 삼성으로선 서두르다 문제 생기면 안 되니….현대차 호조에 도요타가 웃는 이유“현대차의 하이브리드 모델이 더 많이 팔렸으면 좋겠다.” 이것은 현대차 직원이 아니라 경쟁사인 도요타 직원이 한 말. 현대차의 하이브리드 차 판매량은 올 들어 부쩍 증가. 광고를 내보내고 차종을 늘린 결과. 도요타는 브랜드를 막론하고 하이브리드 차가 많이 돌아다녀야 시장이 커진다며 싫지 않은 분위기.같은 동네라도 분양 성적 극과 극분양 바람이 불면 막무가내로 팔려나가던 시절은 지난 듯. 같은 동네 아파트라도 입지와 상품에 따라 분양 성적이 극명하게 엇갈리는 사례가 속출. 가령 위례신도시 ‘위례 2차 엠코타운 센트로엘’은 청약경쟁률이 12.3 대 1, 인근 ‘하남 더샵 센트럴뷰’ 경쟁률은 0.96 대 1. 이젠 ‘묻지마 투자’가 아니라 ‘묻고 투자’.프로골퍼들“우승해도 안 붙네요”</td>\n",
              "      <td>다음달 5일에 영업 정지되는 회사는?</td>\n",
              "      <td>{'text': ['SK텔레콤', 'SK'], 'answer_start': [96, 124]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>c4025bb8e4324f768913ab09092e579c</td>\n",
              "      <td>개인비리로 무너진 원세훈</td>\n",
              "      <td>원세훈 전 국가정보원장(62)이 건설업자로부터 억대 금품을 받은 혐의로 10일 구속 수감됐다. 이명박 전 대통령의 측근 인사가 현 정부 들어 구속된 것은 이번이 처음이다.원 전 원장에 대한 영장실질심사(구속 전 피의자심문)를 담당한 김우수 서울중앙지법 영장전담 부장판사는 이날 밤 10시35분께 “범죄 혐의에 대한 소명이 있고 기록에 비춰 증거인멸 및 도망의 염려가 있다고 보인다”며 구속영장을 발부했다. 이날 오후 중앙지법에 출석한 뒤 서울중앙지검 특수1부에서 대기하던 원 전 원장은 검찰의 영장 집행으로 바로 서울구치소에 수감됐다.법원의 영장 발부로 원 전 원장은 권영해 전 안기부장(김영삼 정부)에 이어 개인 비리로 구속된 두 번째 전직 국정원장이 됐다. 앞서 검찰은 황보건설 대표 황보연 씨(62)로부터 억대 금품을 받고 관급 공사 등을 수주하는 과정에서 편의를 봐준 혐의로 원 전 원장을 조사해 왔다. 그는 2009년 국정원장 취임 후 황씨로부터 여러 차례에 걸쳐 1억5000여만원의 현금과 명품가방, 20돈 순금 십장생 등 수천만원 상당의 선물을 받은 혐의를 받고 있다. 또 뇌물을 받고 황보건설이 삼척그린파워발전소 제2공구 토목공사 등 각종 공사를 따내는 데 편의를 봐준 것으로 검찰은 보고 있다. 검찰은 지난 5일 이 같은 혐의(특정범죄가중처벌법상 알선수재)로 원 전 원장에 대해 구속영장을 청구했다. 원 전 원장은 구속영장 집행 뒤 서울구치소로 향하기 전 ‘현금을 받았다는 혐의에 대해 인정하지 않느냐’는 질문에 고개를 끄덕이며 “네”라고 말했다. 이어 ‘검찰 조사 단계에서 억울하거나 부당한 점이 있었느냐’는 질문에는 “그건 나중에 말하겠다”고 덧붙였다.검찰은 늦어도 이달 말에는 재판에 넘길 방침이다. 정소람 기자 ram@hankyung.com</td>\n",
              "      <td>역사상 처음으로 개인 비리와 관련하여 구속된 전 국정원장의 이름은?</td>\n",
              "      <td>{'text': ['권영해'], 'answer_start': [318]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>cd2098aedac546fd94491628e6caabeb</td>\n",
              "      <td>네이버클라우드, NIA ‘AI 허브 클라우드 확대 도입 운영 사업’ 수주</td>\n",
              "      <td>네이버클라우드(대표 박원기)는 한국지능정보사회진흥원(원장 문용식, 前.한국정보화진흥원, 이하 NIA)의 인공지능 학습용 데이터 구축 사업의 결과물을 개방하기 위한 통합 플랫폼인 AI 허브의 클라우드 자원 확대 및 운용 사업에 대한 계약을 완료했다고 13일 밝혔다. 해당 계약은 지난 10월 디지털 서비스 전문계약제도가 시행된 후 성사된 제1호 계약으로, 기존 공공 부문에서는 이례적으로 민간 클라우드를 도입하여 혁신을 이루고자 하는 NIA의 의지와 네이버클라우드의 기술 역량이 만나 만들어진 최초 사례다. NIA의 인공지능 학습용 데이터 구축 사업은 한국형 뉴딜 사업의 데이터 댐 7대 과제 중 가장 핵심 사업으로, 총 예산이 2020년 2,925억원, 2021년 2,925억원에 달하는 대형 국책사업이며, 오는 2025년까지 1,300여 종의 인공지능 학습용 데이터 구축을 목표로 하고 있다. 또한, 해당 데이터를 네이버 클라우드 플랫폼 기반의 AI 허브에 저장하여 개방할 계획으로 이용자들은 AI 허브를 통해기존 인공지능 학습용 데이터 다운로드 서비스 외에도 다양하게 활용될 수 있는 방법을 제공받을 수 있다고 회사측은 설명했다. 네이버클라우드 임태건 상무는 “네이버클라우드는 인공지능고성능컴퓨팅 임차 사업에 이어, NIA의 인공지능 학습용 데이터 구축 사업의 결과물을 개방하는 AI 허브의 클라우드 자원 확대 사업까지 수주함으로써 명실상부 국가 AI 사업의 핵심적인 역할과 선도적인 지위를 점하게 됐다”며, “대규모 클라우드 분야에 있어 최고의 기술력과 서비스 제공 능력을 인정받게 된 부분에 대해 기쁘게 생각하며, NIA와 함께 대한민국의 AI 경쟁력을 높일 수 있도록 최선을 다하겠다.”고 밝혔다. NIA AI데이터추진단 박정은 단장은 “인공지능 학습용 데이터 구축 사업에 클라우드를 확대 도입하여 인공지능 학습용 데이터와 관련한 보다 나은 서비스 제공이 가능하게 됐다”며, “네이버클라우드와 함께 AI 허브를 성공적으로 개방하여 데이터 댐의 수문을 활짝 열고 디지털 뉴딜을 통한 혁신을 이루고자 한다.”고 전했다. 한편, 네이버클라우드는 최근 정부 및 공공기관이 주도하는 주요 디지털 기반 사업에 클라우드 서비스를 제공하며 유의미한 레퍼런스를 다수 확보해오고 있다. 특히, 이번 코로나19 사태로 한국교육학술정보원과 함께 갑작스럽게 대응해야 했던 온라인 개학에서도 150만 동시 접속 트래픽을 안정적으로 수용할 수 있는 클라우드 환경을 빠르게 구축하여, 공공기관의 대규모 대민 서비스도 클라우드 위에서 안정적으로 운영하는 역량을 높게 평가 받았다.</td>\n",
              "      <td>우리나라의 AI사업 발전을 위해 최선을 다하겠다고 말한 사람의 직책은?</td>\n",
              "      <td>{'text': ['상무'], 'answer_start': [591]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>e47c9f29e4e248d2b868c07178049dc3</td>\n",
              "      <td>질문 3000개 상세히 답한 김충호 사장...“현대車, 고객에게서 답 찾겠다”</td>\n",
              "      <td>김충호 현대자동차 사장(사진)이 “고객으로부터 답을 찾는 기업이 되겠다”고 27일 말했다.김 사장은 이날 서울 양재동 더케이호텔에서 열린 소비자 간담회 ‘마음드림’에 참석해 이같이 밝혔다. 그는 “현대차를 비판하는 목소리를 중요하게 생각하고 있다”며 “고객 목소리에 더욱 귀 기울이고 고객에게서 답을 찾는 기업이 될 것”이라고 강조했다. 이어 “고객의 요구와 기대를 잘 알고 있으며 ‘안티 현대차’ 정서에 대해선 책임을 통감한다”며 “국내 고객의 관심과 성원으로 성장한 기업인 만큼 심기일전해 품질을 더욱 높이고 고객 소통을 확대해 나가겠다”고 덧붙였다.김 사장은 이날 간담회가 열리기 전 소비자들이 올린 3000여개의 질문을 7개 항목으로 분류해 상세히 답변했다. 고객 소통 방식에 대한 생각과 현대차에 대한 부정적 인식, 수입차 증가에 대한 의견, 현대차의 미래 비전 등이 대표적 유형이었다. 김 사장은 현장 참석자들이 던지는 날카로운 질문에도 진솔하게 답변해 참석자들로부터 호평을 받았다. 이어 온라인상에서 현대차에 대해 비판하는 여론을 주의 깊게 보고 개선할 수 있는 사항이 있는지를 검토하겠다는 뜻도 전달했다. 이와 함께 소비자 만족도를 높이기 위해 진행하고 있는 현대차의 노력과 사회적 책임 활동 등을 소개했다.김 사장에 이어 앞으로 권문식 현대차 연구개발본부 부회장과 피터 슈라이어 현대차 디자인총괄 사장, 곽진 현대차 국내영업본부장(부사장) 등도 소비자들을 만나 궁금증을 풀어 줄 계획이다. 현대차 관계자는 “소비자 간담회에서 나오는 모든 의견을 모아 제품 개발과 마케팅 등에 적극 반영할 예정”이라고 말했다.앞서 지난 8월 현대차는 쏘나타 탄생 30주년을 맞아 소비자 300명을 초청해 한국형 쏘나타와 미국형 쏘나타를 직접 충돌시키는 이벤트를 열었다. 한국에서 파는 차량과 미국에서 판매하는 차량의 성능과 사양이 다르다는 오해를 불식시키기 위해서였다. 지난 3월엔 하이브리드차의 배터리가 폭발할 수 있다는 불안감을 없애기 위해 쏘나타 하이브리드차의 후방추돌 시연회를 열었다. 또 공식 블로그를 통해 ‘현대차 에어백은 잘 안 터진다’는 내용을 비롯한 여러 오해를 풀기 위해 노력하고 있다.</td>\n",
              "      <td>현대 자동차가 소비자들의 의견을 듣기 위해 개최한 행사가 열린 장소는?</td>\n",
              "      <td>{'text': ['서울 양재동 더케이호텔', '양재동 더케이호텔', '더케이호텔'], 'answer_start': [59, 62, 66]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>c159422a2fa54702a4a833fbf3a13471</td>\n",
              "      <td>국토부, 코레일 채권한도 상향 검토</td>\n",
              "      <td>정부가 서울 용산국제업무지구 개발 사업 좌초 등으로 자본잠식이 우려되는 코레일(한국철도공사)에 대해 자구노력을 전제조건으로 채권 발행 한도를 늘리는 방안을 검토한다.국토해양부는 코레일이 강도 높은 구조조정을 먼저 추진하는 조건으로 자본금의 두 배인 채권 발행 한도를 상향하는 방안을 고려하고 있다고 13일 발표했다. 하지만 용산역세권개발 사업에 직접적인 개입은 하지 않을 방침이다.구본환 국토부 철도정책국장은 “용산역세권개발 사업은 코레일과 민간이 협약을 맺은 부대사업으로 정부가 개입할 사안이 아니다”며 “정부투자기관관리계획법상 공기업의 경영은 자율에 맡기되 사후 평가는 가능하다”고 말했다. 그는 또 “드림허브가 파산이 돼도 철도 운영은 큰 영향을 받지 않을 것”이라고 설명했다. 코레일은 용산역세권 개발 사업 부도에 따른 자본잠식 가능성이 있지만 역사 철도부지 등의 자산을 재평가하면 2조8000억원가량의 자본금 증대 효과가 있다. 따라서 자본잠식에 빠질 가능성은 낮다는 게 국토부의 설명이다.국토부는 이와 함께 코레일의 채권 발행 한도를 늘려줄 수 있다는 입장을 밝혔다. 현재 코레일의 채권 발행 한도는 자본금의 2배로 도로공사(4배)와 LH(10배) 등에 비해 적은 편이다. 철도 설비 공사비 등 부채는 총 11조원 규모다.</td>\n",
              "      <td>용산역세권개발 사업을 맡은 기업보다 채권 발행 한도 기준이 2배 높은 기업은?</td>\n",
              "      <td>{'text': ['도로공사'], 'answer_start': [575]}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>a818ea2cb0fd4e8e8e3828dcfd3592fc</td>\n",
              "      <td>이달 압구정동에 ‘리저브’ 1호점 … 고객이 고른 원두로 바리스타 즉석 제조</td>\n",
              "      <td>스타벅스가 미국 일본 등 몇 개 국가에서만 운영하고 있는 럭셔리 매장인 ‘스타벅스 리저브’를 서울 압구정동에 연다. 스타벅스 리저브는 고객이 현장에서 선택한 원두를 커피로 내려 마실 수 있는 게 특징이며 일반 매장에 비해 가격이 두세 배 비싸다. CJ푸드빌의 투썸플레이스도 이달 초 서울 신사동에 럭셔리 매장을 열 계획이다. 지난해 문을 연 탐앤탐스의 ‘더 칼립소’ 등과 함께 커피전문점의 고급화 경쟁이 뜨거워질 전망이다.스타벅스는 오는 18일 리저브 매장을 공개하기로 하고 커피 전문가들에게 최근 초청장을 발송했다. 스타벅스는 이르면 이달 말 매장을 정식 개장할 방침이다. 올해 5~6개의 리저브 매장을 국내에 더 낼 것이라고 밝혔다.스타벅스의 일반 매장은 전 세계에 1만9200개에 달하지만 리저브 매장은 500곳에 불과하다. 파나마 카메룬 등 해외 유명 커피 산지의 최고급 원두로 커피를 만든다. 고객이 원두를 선택하면 현장에서 갈아 커피를 만들어준다. 바리스타는 고객을 1 대 1로 맡아 원두의 특성과 제조 과정 등을 설명해준다. 리저브 매장에는 한 번에 한 잔의 커피만 뽑아내는 고급 에스프레소 머신인 ‘클로버 시스템’이 설치돼 있다. 커피 가격은 일반 매장에 비해 두세 배 비싸다. 작년 미국 스타벅스 리저브 매장에서 가장 비싸게 팔린 커피는 ‘게이샤’ 품종의 원두로 만든 ‘코스타리카 핀카 파밀레라’였다. 그란데 사이즈(473mL) 한 잔이 7달러로 일반 매장에 비해 3.2배 비싸게 팔렸다. 국가 간 커피 가격에 차이가 있지만 국내 리저브 매장에서 파는 커피 한 잔 값은 1만원 안팎이 될 것으로 보인다. 스타벅스 관계자는 “스타벅스 글로벌에서 벌이는 고급화 전략의 일환으로 한국에 리저브 매장을 내는 것”이라고 밝혔다.CJ푸드빌은 이르면 7일 가로수길 초입에 투썸플레이스 고급화 매장을 연다. 투썸플레이스 가로수길점을 리뉴얼한 이 매장은 새로운 이름을 만드는 대신 ‘프리미엄 디저트 카페’라는 기존 브랜드 콘셉트에 집중키로 했다고 회사 측은 설명했다. CJ푸드빌은 에그타르트 등 디저트 메뉴를 강화할 방침이다. 또 몇 가지 간식 메뉴와 차를 함께 즐기는 ‘애프터눈 티’를 중점적으로 판매할 계획이다.CJ 역시 고객들이 원두와 추출 도구를 선택할 수 있도록 할 방침이다. 생산지, 품종, 생산 과정이 투명하다는 인증을 받은 ‘싱글 오리진’ 원두만 사용키로 했다.탐앤탐스가 지난해 6월 압구정 로데오거리에 연 ‘더 칼립소’에서는 동티모르, 에티오피아, 쿠바 등의 원두로 만든 커피를 7000~1만500원에 팔고 있다. 일반 매장의 아메리카노(3800원)보다 두세 배 비싼 값이다. 할리스커피의 고급 매장인 가로수길점에서도 이 같은 싱글오리진 커피를 맛볼 수 있다.커피전문점들이 고급화 매장 경쟁에 뛰어든 것은 국내 커피 시장이 포화 상태에 이르면서 차별화를 시도하려는 전략으로 분석된다. 커피전문점 창업 트렌드를 분석하는 fc창업코리아의 강병오 대표는 “미국 등에선 원두 고유의 맛을 살린 커피가 인기를 끌고 있다”며 “고급화 경쟁은 결국 커피 맛의 싸움으로 귀결될 것”이라고 말했다.</td>\n",
              "      <td>리저브 매장에서 판매가가 최고 비싼 커피의 원두는?</td>\n",
              "      <td>{'text': ['‘게이샤’', '게이샤'], 'answer_start': [646, 647]}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl6IidfdIrJK"
      },
      "source": [
        "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the 🤗 Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "5VVbsJL7dKuG"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-iDiK42dKuG"
      },
      "source": [
        "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rowT4iCLIrJK"
      },
      "source": [
        "You can directly call this tokenizer on two sentences (one for the answer, one for the context):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "a5hBlsrHIrJL",
        "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [699, 402, 442, 517, 412, 517, 458, 427, 450, 517, 425, 373, 389, 258, 0, 671, 458, 517, 425, 373, 389, 517, 412, 687, 458, 420, 453, 367, 409, 54, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2tmHiUNdKuG"
      },
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
        "\n",
        "Now one specific thing for the preprocessing in question answering is how to deal with very long documents. We usually truncate them in other tasks, when they are longer than the model maximum sentence length, but here, removing part of the the context might result in losing the answer we are looking for. To deal with this, we will allow one (long) example in our dataset to give several input features, each of length shorter than the maximum length of the model (or the one we set as a hyper-parameter). Also, just in case the answer lies at the point we split a long context, we allow some overlap between the features we generate controlled by the hyper-parameter `doc_stride`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "uk4HZC0gdKuG"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfW44OpfdKuG"
      },
      "source": [
        "Let's find one long example in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "BO2Wpf6OdKuG"
      },
      "outputs": [],
      "source": [
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
        "        break\n",
        "example = datasets[\"train\"][i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zZwb42mdKuH"
      },
      "source": [
        "Without any truncation, we get the following length for the input IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "M-WAUEBrdKuH",
        "outputId": "b8772b02-6a8d-470c-ed8d-96fe690d6113"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2105"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8BoRBcVdKuH"
      },
      "source": [
        "Now, if we just truncate, we will lose information (and possibly the answer to our question):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "retnGv27dKuH",
        "outputId": "a97dc1de-2f2c-4e40-c4a3-2be20006a284"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jf6fFE-dKuH"
      },
      "source": [
        "Note that we never want to truncate the question, only the context, else the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and by passing the stride:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "4IVjkRD4dKuH"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTuZB0JXdKuH"
      },
      "source": [
        "Now we don't have one list of `input_ids`, but several: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "kVjQoGoKdKuH",
        "outputId": "ab357b6d-552e-4441-e76e-1ae5580b93e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[384, 384, 384, 384, 384, 384, 384, 384, 297]"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqVDvmNOdKuI"
      },
      "source": [
        "And if we decode them, we can see the overlap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "tPCUleozdKuI",
        "outputId": "325d9b02-fd59-4323-d0a7-7b0777d5e113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ᄋ[UNK]ᄋ[UNK]ᄋ[UNK] ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᅵᄋ[UNK] ᄀ[UNK]ᄒ[UNK] [UNK]ᄀ[UNK]?[UNK] ‘[UNK]ᅵ[UNK]ᄋ[UNK]ᄒ[UNK]ᄒ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK]ᄀ[UNK]ᄒ[UNK], [UNK] ᄋ[UNK]ᄒ[UNK] ᄋ[UNK], [UNK]ᅵ[UNK]ᄒ[UNK]ᄀ[UNK], [UNK] [UNK]ᄀ[UNK] [UNK] ᄋ[UNK]ᄋ[UNK], ᄀ[UNK]ᄀ[UNK] ᄀ[UNK] [UNK] [UNK]ᄀ[UNK] [UNK]ᅵᄋ[UNK]....’ ᄀ[UNK]ᄋ[UNK]ᄀ[UNK]ᄋ[UNK]ᄋ[UNK] ‘[UNK]ᄋ[UNK]’[UNK] [UNK]ᅵ[UNK] 기ᄒ[UNK]ᄀ[UNK]ᄀ[UNK] [UNK]이 [UNK]ᅵ[UNK] [UNK] 156[UNK]ᄋ[UNK] [UNK]ᄀ[UNK] ᄀ[UNK]ᄋ[UNK]ᄋ[UNK] [UNK]이[UNK]ᄒ[UNK] ᄋ[UNK]ᄋ[UNK] [UNK] [UNK]ᄀ[UNK] ᄒ[UNK] 이[UNK]ᄀ[UNK]ᄋ[UNK] [UNK]ᄒ[UNK] [UNK]ᄀ[UNK] [UNK]이[UNK] ᄀ[UNK]ᄀ[UNK]. ‘ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᅵ [UNK]ᄒ[UNK]’[UNK] [UNK]이 [UNK]ᄋ[UNK] [UNK]. ○‘[UNK]ᅵ[UNK]ᄒ[UNK]’ [UNK]ᄀ[UNK] ᄋ[UNK]ᄒ[UNK] [UNK]ᅵ[UNK]ᄀ[UNK]ᄀ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK] ᄀ[UNK]ᄀ[UNK] ᄋ[UNK] [UNK] ᄒ[UNK]ᄀ[UNK] [UNK] [UNK]ᄀ[UNK], [UNK] ᄒ[UNK]기[UNK]ᅵ [UNK] 이[UNK]ᄀ[UNK] 기[UNK]ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᄒ[UNK](ᄀ[UNK]ᄋ[UNK]) [UNK]ᅵ[UNK]이 [UNK]ᄋ[UNK] 이[UNK] [UNK]ᅵ[UNK]ᄒ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK] ᄒ[UNK]ᄋ[UNK]ᄒ[UNK]. [UNK]ᅵ[UNK]ᄒ[UNK]ᄋ[UNK] ᄀ[UNK]([UNK])[UNK] [UNK]이 [UNK] [UNK]ᅵ[UNK][UNK][UNK]\n",
            "ᄋ[UNK]ᄋ[UNK]ᄋ[UNK] ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᅵᄋ[UNK] ᄀ[UNK]ᄒ[UNK] [UNK]ᄀ[UNK]?[UNK]ᅵ[UNK]ᄀ[UNK]ᄀ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK] ᄀ[UNK]ᄀ[UNK] ᄋ[UNK] [UNK] ᄒ[UNK]ᄀ[UNK] [UNK] [UNK]ᄀ[UNK], [UNK] ᄒ[UNK]기[UNK]ᅵ [UNK] 이[UNK]ᄀ[UNK] 기[UNK]ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᄒ[UNK](ᄀ[UNK]ᄋ[UNK]) [UNK]ᅵ[UNK]이 [UNK]ᄋ[UNK] 이[UNK] [UNK]ᅵ[UNK]ᄒ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK] ᄒ[UNK]ᄋ[UNK]ᄒ[UNK]. [UNK]ᅵ[UNK]ᄒ[UNK]ᄋ[UNK] ᄀ[UNK]([UNK])[UNK] [UNK]이 [UNK] [UNK]ᅵ[UNK]ᅵ [UNK]ᄋ[UNK] [UNK]ᄋ[UNK] [UNK]ᄒ[UNK]ᄋ[UNK] [UNK] [UNK] 이 [UNK]ᄋ[UNK] ᄀ[UNK] [UNK]이[UNK]ᄋ[UNK] [UNK] ᄀ[UNK] ᄀ[UNK]. [UNK] [UNK]ᄀ[UNK] [UNK] ᄀ[UNK]ᄋ[UNK]ᄒ[UNK] ᄒ[UNK]ᄒ[UNK]. ᄋ[UNK] [UNK] ᄒ[UNK] 이[UNK]ᄀ[UNK] 이 [UNK]ᄋ[UNK] [UNK]. ᄋ[UNK] [UNK] ᄒ[UNK] 이[UNK]ᄀ[UNK] 4[UNK]ᄋ[UNK] [UNK]ᄒ[UNK] 이ᄋ[UNK] ᄋ[UNK]이 2006[UNK] 2012[UNK]ᅵ ᄀ[UNK]ᄋ[UNK] [UNK]ᅵ[UNK]이 [UNK]ᄋ[UNK] [UNK]ᅵ[UNK]ᄒ[UNK] ᄒ[UNK] ᄀ[UNK]ᄋ[UNK] 66ᄋ[UNK]ᄋ[UNK]ᄋ[UNK] [UNK]ᄀ[UNK]ᄋ[UNK]. ᄀ[UNK] ᄀ[UNK]ᄀ[UNK]ᄋ[UNK] 기ᄒ[UNK]ᄀ[UNK]ᄀ[UNK]ᄋ[UNK] “[UNK]기[UNK] [UNK]ᄀ[UNK]ᄋ[UNK] ᄋ[UNK]ᅵᄋ[UNK]ᅵ[UNK] [UNK]히 기[UNK] 기ᄀ[UNK] [UNK][UNK][UNK]\n"
          ]
        }
      ],
      "source": [
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C4WwfnEdKuI"
      },
      "source": [
        "Now this will give us some work to properly treat the answers: we need to find in which of those features the answer actually is, and where exactly in that feature. The models we will use require the start and end positions of these answers in the tokens, so we will also need to to map parts of the original context to some tokens. Thankfully, the tokenizer we're using can help us with that by returning an `offset_mapping`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "_GFA_MYTdKuI",
        "outputId": "18cb6fe2-9061-48c7-c5cb-f6cadeab8a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(0, 1), (0, 1), (0, 2), (2, 3), (2, 3), (3, 4), (3, 4), (4, 5), (5, 6), (5, 6), (6, 7), (6, 8), (7, 8), (8, 9), (8, 9), (9, 10), (9, 10), (10, 11), (11, 12), (11, 13), (13, 14), (13, 14), (14, 15), (15, 16), (16, 17), (16, 20), (20, 21), (0, 0), (0, 1), (1, 2), (1, 2), (1, 2), (2, 3), (2, 3), (3, 4), (3, 6), (6, 7), (6, 7), (7, 8), (7, 8), (8, 9), (9, 10), (10, 11), (10, 11), (11, 12), (11, 12), (12, 13), (12, 13), (13, 14), (14, 15), (15, 17), (17, 18), (18, 19), (18, 19), (19, 20), (19, 20), (20, 21), (21, 22), (21, 23), (23, 24), (24, 25), (25, 27), (26, 27), (26, 27), (27, 28), (27, 28), (28, 29), (28, 29), (29, 30), (30, 31), (31, 33), (33, 34), (34, 35), (35, 36), (35, 36), (36, 37), (37, 39), (39, 40), (40, 41), (40, 41), (41, 42), (41, 42), (42, 43), (43, 44), (44, 45), (44, 45), (45, 46), (45, 47), (47, 48), (48, 49), (48, 49), (49, 50), (50, 52), (52, 53), (53, 54), (54, 55), (54, 55), (55, 56), (56, 57), (56, 57)]\n"
          ]
        }
      ],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "Wd2NAV54dKuI",
        "outputId": "da9511fb-6b25-4cc8-c8c2-d2f0e5f58d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ᄋ 유\n"
          ]
        }
      ],
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVwhS9CvdKuJ"
      },
      "source": [
        "So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "_NVHVQkYdKuJ",
        "outputId": "0d4d0ffb-fa8e-41df-be69-4f0b6580f968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None, None]\n"
          ]
        }
      ],
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "print(sequence_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN41fCrYdKuJ"
      },
      "source": [
        "It returns `None` for the special tokens, then 0 or 1 depending on whether the corresponding token comes from the first sentence past (the question) or the second (the context). Now with all of this, we can find the first and last token of the answer in one of our input feature (or if the answer is not in this feature):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "eNgv8LAVdKuJ",
        "outputId": "d1668b30-712d-4ba3-a16a-9eb3a157b8dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "307 315\n"
          ]
        }
      ],
      "source": [
        "answers = example[\"answers\"]\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# Start token index of the current span in the text.\n",
        "token_start_index = 0\n",
        "while sequence_ids[token_start_index] != 1:\n",
        "    token_start_index += 1\n",
        "\n",
        "# End token index of the current span in the text.\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "while sequence_ids[token_end_index] != 1:\n",
        "    token_end_index -= 1\n",
        "\n",
        "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
        "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "        token_start_index += 1\n",
        "    start_position = token_start_index - 1\n",
        "    while offsets[token_end_index][1] >= end_char:\n",
        "        token_end_index -= 1\n",
        "    end_position = token_end_index + 1\n",
        "    print(start_position, end_position)\n",
        "else:\n",
        "    print(\"The answer is not in this feature.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ8YDBj9dKuJ"
      },
      "source": [
        "And we can double check that it is indeed the theoretical answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "rcer0uK2dKuK",
        "outputId": "d12917c5-fac4-40c6-82a2-b5a7d0610832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ᅵ[UNK]ᄀ[UNK]ᄋ[UNK]ᅵ[UNK]ᄒ\n",
            "기독교복음침례회\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
        "print(answers[\"text\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhZ_s2fBdKuK"
      },
      "source": [
        "For this notebook to work with any kind of models, we need to account for the special case where the model expects padding on the left (in which case we switch the order of the question and the context):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61IE61FodKuK"
      },
      "source": [
        "Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position. We could also simply discard those examples from the training set if the flag `allow_impossible_answers` is `False`. Since the preprocessing is already complex enough as it is, we've kept is simple for this part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "eMyddpCrdKuK"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lm8ozrJIrJR"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command. Since our preprocessing changes the number of samples, we need to remove the old columns when applying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b164942fced74893a05f4cf48b5236bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/11 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "2 is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25132\\1625105510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare_train_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    775\u001b[0m             \u001b[0mcache_file_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         return DatasetDict(\n\u001b[1;32m--> 777\u001b[1;33m             {\n\u001b[0m\u001b[0;32m    778\u001b[0m                 k: dataset.map(\n\u001b[0;32m    779\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    776\u001b[0m         return DatasetDict(\n\u001b[0;32m    777\u001b[0m             {\n\u001b[1;32m--> 778\u001b[1;33m                 k: dataset.map(\n\u001b[0m\u001b[0;32m    779\u001b[0m                     \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m                     \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m             return self._map_single(\n\u001b[0m\u001b[0;32m   2586\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m         }\n\u001b[0;32m    551\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2980\u001b[0m                         )  # Something simpler?\n\u001b[0;32m   2981\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2982\u001b[1;33m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[0;32m   2983\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2984\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2863\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2865\u001b[1;33m             \u001b[0mprocessed_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2866\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2867\u001b[0m                 \u001b[1;31m# Check if the function returns updated examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\sms20\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2543\u001b[0m                 )\n\u001b[0;32m   2544\u001b[0m                 \u001b[1;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2545\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2546\u001b[0m                 \u001b[1;31m# Return a standard dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2547\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25132\\3517064630.py\u001b[0m in \u001b[0;36mprepare_train_features\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# We will label impossible answers with the index of the CLS token.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_examples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mcls_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Grab the sequence corresponding to that example (to know what is the context and what is the question).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: 2 is not in list"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voWiw8C7IrJV"
      },
      "source": [
        "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready for training, we can download the pretrained model and fine-tune it. Since our task is question answering, we use the `AutoModelForQuestionAnswering` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW",
        "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CczA5lJlIrJX"
      },
      "source": [
        "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N8urzhyIrJY"
      },
      "source": [
        "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
        "\n",
        "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-squad\"` or `\"huggingface/bert-finetuned-squad\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNyo3qDgdKuM"
      },
      "source": [
        "Then we will need a data collator that will batch our processed examples together, here the default one will work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfu9Ou-MdKuM"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "We will evaluate our model and compute metrics in the next section (this is a very long operation, so we will only compute the evaluation loss during training).\n",
        "\n",
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='16599' max='16599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16599/16599 58:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.220600</td>\n",
              "      <td>1.160322</td>\n",
              "      <td>39.574900</td>\n",
              "      <td>272.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.945200</td>\n",
              "      <td>1.121690</td>\n",
              "      <td>39.706000</td>\n",
              "      <td>271.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.773000</td>\n",
              "      <td>1.157358</td>\n",
              "      <td>39.734000</td>\n",
              "      <td>271.405000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16599, training_loss=1.1112074395519933, metrics={'train_runtime': 3487.0114, 'train_samples_per_second': 4.76, 'total_flos': 40606919924189184, 'epoch': 3.0})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FoL1DT5dKuM"
      },
      "source": [
        "Since this training is particularly long, let's save the model just in case we need to restart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZNCgEx_dKuM"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OzSKpEidKuN"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZGzJ7V8dKuN"
      },
      "source": [
        "Evaluating our model will require a bit more work, as we will need to map the predictions of our model back to parts of the context. The model itself predicts logits for the start and en position of our answers: if we take a batch from our validation datalaoder, here is the output our model gives us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQjNylhqdKuN",
        "outputId": "429a078a-a450-4a8d-c315-e6e027c88dc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'start_logits', 'end_logits'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8lSElpzdKuN"
      },
      "source": [
        "The output of the model is a dict-like object that contains the loss (since we provided labels), the start and end logits. We won't need the loss for our predictions, let's have a look a the logits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QUVn8ATdKuN",
        "outputId": "3f028be1-6b1e-49ad-b5e9-20f31e86240d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([16, 384]), torch.Size([16, 384]))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.start_logits.shape, output.end_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLsM4wG5dKuN"
      },
      "source": [
        "We have one logit for each feature and each token. The most obvious thing to predict an answer for each featyre is to take the index for the maximum of the start logits as a start position and the index of the maximum of the end logits as an end position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0884VUvdKuO",
        "outputId": "883e3577-d546-4735-b8f0-d3a470ba2455"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91,\n",
              "         156,  35], device='cuda:0'),\n",
              " tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94,\n",
              "         158,  35], device='cuda:0'))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjBDHaw1dKuO"
      },
      "source": [
        "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
        "\n",
        "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "\n",
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call `n_best_size`. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lcEoRDedKuO"
      },
      "outputs": [],
      "source": [
        "n_best_size = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljmYX4jqdKuO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
        "                }\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSgJBk2mdKuO"
      },
      "source": [
        "And then we can sort the `valid_answers` according to their `score` and only keep the best one. The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "- the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "- the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from `prepare_train_features`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghV3qnXzdKuO"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNXNcmyIdKuO"
      },
      "source": [
        "And like before, we can apply that function to our validation set easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "32ba04d6240149f49eb48c8d8b7f9aae"
          ]
        },
        "id": "ljpLmpk7dKuO",
        "outputId": "4c0d1c08-ba66-4c9b-9793-fd2c7ea6c104"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32ba04d6240149f49eb48c8d8b7f9aae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTkHCDrXdKuP"
      },
      "source": [
        "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PULRWPQXdKuP",
        "outputId": "25d38950-82d0-4155-f2ef-264dbc3fc557"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [674/674 00:39]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Kqac8KdKuP"
      },
      "source": [
        "The `Trainer` *hides* the columns that are not used by the model (here `example_id` and `offset_mapping` which we will need for our post-processing), so we set them back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VV9XD1UdKuP"
      },
      "outputs": [],
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gLWJpHmdKuP"
      },
      "source": [
        "We can now refine the test we had before: since we set `None` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVrCqy66dKuP"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFuLDJOwdKuP",
        "outputId": "d01036e6-8152-4299-99ef-43342fa50933"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 16.706663, 'text': 'Denver Broncos'},\n",
              " {'score': 14.635585,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 13.234194, 'text': 'Carolina Panthers'},\n",
              " {'score': 12.468662, 'text': 'Broncos'},\n",
              " {'score': 11.709289, 'text': 'Denver'},\n",
              " {'score': 10.397583,\n",
              "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 10.104669,\n",
              "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
              " {'score': 9.721636,\n",
              "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
              " {'score': 9.007437,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10'},\n",
              " {'score': 8.834958,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
              " {'score': 8.38701,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
              " {'score': 8.143825,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
              " {'score': 8.03359,\n",
              "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 7.832466,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
              " {'score': 7.650557,\n",
              "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
              " {'score': 7.6060467, 'text': 'Carolina Panthers 24–10'},\n",
              " {'score': 7.5795317,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
              " {'score': 7.433568, 'text': 'Carolina'},\n",
              " {'score': 6.742434,\n",
              "  'text': 'Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
              " {'score': 6.71136,\n",
              "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start_logits = output.start_logits[0].cpu().numpy()\n",
        "end_logits = output.end_logits[0].cpu().numpy()\n",
        "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
        "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
        "# an example index\n",
        "context = datasets[\"validation\"][0][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "        # to part of the input_ids that are not in the context.\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or offset_mapping[start_index] is None\n",
        "            or offset_mapping[end_index] is None\n",
        "        ):\n",
        "            continue\n",
        "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            start_char = offset_mapping[start_index][0]\n",
        "            end_char = offset_mapping[end_index][1]\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": context[start_char: end_char]\n",
        "                }\n",
        "            )\n",
        "\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCwxuW2ddKuQ"
      },
      "source": [
        "We can compare to the actual ground-truth answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6MSjfkYdKuQ",
        "outputId": "c3d49420-4bff-4270-cd3f-c8d3bc39f56d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'answer_start': [177, 177, 177],\n",
              " 'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets[\"validation\"][0][\"answers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oazzlKedKuQ"
      },
      "source": [
        "Our model picked the right as the most likely answer!\n",
        "\n",
        "As we mentioned in the code above, this was easy on the first feature because we knew it comes from the first example. For the other features, we will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2tC76xEdKuQ"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "examples = datasets[\"validation\"]\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5K01lVdKuQ"
      },
      "source": [
        "We're almost ready for our post-processing function. The last bit to deal with is the impossible answer (when `squad_v2 = True`). The code above only keeps answers that are inside the context, we need to also grab the score for the impossible answer (which has start and end indices corresponding to the index of the CLS token). When one example gives several features, we have to predict the impossible answer when all the features give a high score to the impossible answer (since one feature could predict the impossible answer just because the answer isn't in the part of the context it has access too), which is why the score of the impossible answer for one example is the *minimum* of the scores for the impossible answer in each feature generated by the example.\n",
        "\n",
        "We then predict the impossible answer when that score is greater than the score of the best non-impossible answer. All combined together, this gives us this post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvRWJQyldKuQ"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Logging.\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if not squad_v2:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "        else:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTNw_9JydKuQ"
      },
      "source": [
        "And we can apply our post-processing function to our raw predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "347ebed36d3541388e4e821372e91aa4"
          ]
        },
        "id": "46ZvrcZOdKuQ",
        "outputId": "f1042ab5-76c9-4706-e9d0-1d564647022b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-processing 10570 example predictions split into 10784 features.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "347ebed36d3541388e4e821372e91aa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2zHwImHdKuQ"
      },
      "source": [
        "Then we can load the metric from the datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-scg-4-dKuR"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5dNI4ADdKuR"
      },
      "source": [
        "Then we can call compute on it. We just need to format predictions and labels a bit as it expects a list of dictionaries and not one big dictionary. In the case of squad_v2, we also have to set a `no_answer_probability` argument (which we set to 0.0 here as we have already set the answer to empty if we picked it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUNTXLLtdKuR",
        "outputId": "52721339-141c-4b51-82fc-e17b90c32cb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'exact_match': 76.74550614947965, 'f1': 85.13412652023338}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if squad_v2:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
        "else:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0nHzpDhdKuR"
      },
      "source": [
        "You can now upload the result of the training to the Hub, just execute this instruction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfDwoAzkdKuR"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I14Wht2IdKuR"
      },
      "source": [
        "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
        "\n",
        "```python\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"sgugger/my-awesome-model\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96MC6GwfdKuR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8aa1ea3e156bda037fa3e6423dab0ca647e35b6e88545446200f40693df29f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
